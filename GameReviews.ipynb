{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d1a882f-268f-417b-b259-52c812392009",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched reviews. New cursor: AoIIPwYYanDj7egE\n",
      "Fetched reviews. New cursor: AoIIPwFUIn744ekE\n",
      "Fetched reviews. New cursor: AoIIPwAAAHb2gOcE\n",
      "Fetched reviews. New cursor: AoIIPwwLVnSJ6egE\n",
      "Fetched reviews. New cursor: AoIIPwVCon%2Bo3uwE\n",
      "Fetched reviews. New cursor: AoIIPwAAAHb2gOcE\n",
      "Fetched reviews. New cursor: AoIIPwwLVnSJ6egE\n",
      "Fetched reviews. New cursor: AoIIPwVCon%2Bo3uwE\n",
      "Fetched reviews. New cursor: AoIIPwAAAHb2gOcE\n",
      "Fetched reviews. New cursor: AoIIPwwLVnSJ6egE\n",
      "Fetched reviews. New cursor: AoIIPwVCon%2Bo3uwE\n",
      "Fetched reviews. New cursor: AoIIPwAAAHb2gOcE\n",
      "Fetched reviews. New cursor: AoIIPwwLVnSJ6egE\n",
      "Fetched reviews. New cursor: AoIIPwVCon%2Bo3uwE\n",
      "Fetched reviews. New cursor: AoIIPwAAAHb2gOcE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of reviews: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(reviewData))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 36\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mgetApiSteam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Load the review data from the file\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviewdata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m, in \u001b[0;36mgetApiSteam\u001b[1;34m(repeat, num_per_page, url, cursor, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetched reviews. New cursor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_cursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Recursively call the function with the updated cursor and data\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetApiSteam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_per_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m, in \u001b[0;36mgetApiSteam\u001b[1;34m(repeat, num_per_page, url, cursor, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetched reviews. New cursor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_cursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Recursively call the function with the updated cursor and data\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetApiSteam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_per_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: getApiSteam at line 33 (12 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m, in \u001b[0;36mgetApiSteam\u001b[1;34m(repeat, num_per_page, url, cursor, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetched reviews. New cursor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_cursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Recursively call the function with the updated cursor and data\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetApiSteam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_per_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m, in \u001b[0;36mgetApiSteam\u001b[1;34m(repeat, num_per_page, url, cursor, data)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetApiSteam\u001b[39m(repeat, num_per_page\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://store.steampowered.com/appreviews/892970?json=1&language=english\u001b[39m\u001b[38;5;124m\"\u001b[39m, cursor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_per_page\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m repeat \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Flatten the list of reviews\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         data \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib.parse\n",
    "import time\n",
    "\n",
    "def getApiSteam(repeat, num_per_page=20, url=\"https://store.steampowered.com/appreviews/892970?json=1&language=english\", cursor='*', data=[]):\n",
    "    time.sleep(num_per_page/10)\n",
    "    \n",
    "    if repeat == 0:\n",
    "        # Flatten the list of reviews\n",
    "        data = [x for xs in data for x in xs]\n",
    "        # Extract only the review text\n",
    "        reviewData = [i[\"review\"] for i in data]\n",
    "        \n",
    "        # Save the review data to a file\n",
    "        with open('reviewdata.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(reviewData, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        return 1\n",
    "    else:\n",
    "        repeat -= 1\n",
    "\n",
    "        # Fetch the next page of reviews\n",
    "        response = requests.get(f'{url}&cursor={cursor}&num_per_page={num_per_page}')\n",
    "        temp = json.loads(response.text)\n",
    "        # Append the new reviews to the data list\n",
    "        data.append(temp[\"reviews\"])\n",
    "        # Update the cursor for the next request\n",
    "        new_cursor = urllib.parse.quote(temp[\"cursor\"])\n",
    "        print(f\"Fetched reviews. New cursor: {new_cursor}\")\n",
    "        \n",
    "        # Recursively call the function with the updated cursor and data\n",
    "        return getApiSteam(repeat, num_per_page, cursor=new_cursor, data=data)\n",
    "\n",
    "def main():\n",
    "    getApiSteam(20, 50)\n",
    "    # Load the review data from the file\n",
    "    with open(\"reviewdata.json\", \"r\", encoding='utf-8') as file:\n",
    "        reviewData = json.load(file)\n",
    "    print(\"Number of reviews: \", len(reviewData))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e6aa5a7a-49d0-4f32-a9b4-5c3ce26a8911",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed:\n",
      "took three try get game felt unnatural first sat put two three hour game immediately hooked like basically game ever writing 383 hour later even dove playing game friend yet absolute best part game according take space drive cost le aaa game find absolutely delightful exploration amazing genuinely love world natural difficulty make actually care preparing cooking lose stats adjusted game definitely within top ten game time\n",
      "\n",
      "\n",
      "Not Processed:\n",
      "It took me three tries to get into this game - it felt unnatural to me at first, but after I sat down and put about two to three hours into the game it immediately hooked me like basically no other game ever has. \n",
      "\n",
      "I am writing this 383 hours later, and I haven't even dove into playing the game with my friends yet (The absolute best part of the game according to most). It takes no space on your drive, it costs less than all AAA games, and I find it absolutely delightful.\n",
      "\n",
      "Exploration is amazing - I genuinely love this world.\n",
      "Natural difficulty makes me *actually care* about preparing, about cooking, so I don't lose stats (Can be adjusted)\n",
      "\n",
      "This game is definitely within my top ten games of all time now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\reapy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\reapy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\reapy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read Game Review Data JSON\n",
    "with open(\"reviewdata.json\",\"r\", encoding='utf-8') as file:\n",
    "        reviewData = json.load(file)\n",
    "    \n",
    "def preprocess(text):\n",
    "    # Convert text to lowercase\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    \n",
    "    # Removing punctuation and special characters\n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    \n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "# Joining tokens back into a string\n",
    "preprocessed_text = ' '.join(preprocess(reviewData[22]))\n",
    "\n",
    "print(\"\\nProcessed:\\n\"+preprocessed_text)\n",
    "print(\"\\n\\nNot Processed:\\n\"+reviewData[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e5e99-6b40-4dc8-a762-b76ae925fafb",
   "metadata": {},
   "source": [
    "### Get Steam Reviews JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15453d10-029b-4178-b5c6-adb657d270ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading idlist.txt\n",
      "Loading idprocessed_on_20240422.txt\n",
      "Creating idprocessed_on_20240422.txt\n",
      "Downloading reviews for appID = 1623730\n",
      "[appID = 1623730] expected #reviews = 124655\n",
      "[appID = 1623730] num_reviews = 5349 (expected: 124655)\n",
      "Downloading reviews for appID = 892970\n",
      "[appID = 892970] expected #reviews = 262477\n",
      "[appID = 892970] num_reviews = 600 (expected: 262477)\n",
      "Downloading reviews for appID = 105600\n",
      "[appID = 105600] expected #reviews = 636028\n",
      "[appID = 105600] num_reviews = 1077 (expected: 636028)\n",
      "Downloading reviews for appID = 346110\n",
      "[appID = 346110] expected #reviews = 304609\n",
      "[appID = 346110] num_reviews = 398 (expected: 304609)\n",
      "Downloading reviews for appID = 252490\n",
      "[appID = 252490] expected #reviews = 532677\n",
      "[appID = 252490] num_reviews = 1369 (expected: 532677)\n",
      "Downloading reviews for appID = 275850\n",
      "[appID = 275850] expected #reviews = 176517\n",
      "[appID = 275850] num_reviews = 694 (expected: 176517)\n",
      "Downloading reviews for appID = 322330\n",
      "[appID = 322330] expected #reviews = 116595\n",
      "[appID = 322330] num_reviews = 301 (expected: 116595)\n",
      "Downloading reviews for appID = 648800\n",
      "[appID = 648800] expected #reviews = 126602\n",
      "[appID = 648800] num_reviews = 361 (expected: 126602)\n",
      "Downloading reviews for appID = 251570\n",
      "[appID = 251570] expected #reviews = 146943\n",
      "[appID = 251570] num_reviews = 501 (expected: 146943)\n",
      "Downloading reviews for appID = 211820\n",
      "[appID = 211820] expected #reviews = 72511\n",
      "[appID = 211820] num_reviews = 209 (expected: 72511)\n",
      "Downloading reviews for appID = 242760\n",
      "[appID = 242760] expected #reviews = 184817\n",
      "[appID = 242760] num_reviews = 389 (expected: 184817)\n",
      "Downloading reviews for appID = 513710\n",
      "[appID = 513710] expected #reviews = 40476\n",
      "[appID = 513710] num_reviews = 319 (expected: 40476)\n",
      "Downloading reviews for appID = 108600\n",
      "[appID = 108600] expected #reviews = 129309\n",
      "[appID = 108600] num_reviews = 875 (expected: 129309)\n",
      "Downloading reviews for appID = 440900\n",
      "[appID = 440900] expected #reviews = 49582\n",
      "[appID = 440900] num_reviews = 290 (expected: 49582)\n",
      "Downloading reviews for appID = 1149460\n",
      "[appID = 1149460] expected #reviews = 22485\n",
      "Number of queries 150 reached. Cooldown: 310 seconds\n",
      "[appID = 1149460] num_reviews = 189 (expected: 22485)\n",
      "Downloading reviews for appID = 264710\n",
      "[appID = 264710] expected #reviews = 152808\n",
      "[appID = 264710] num_reviews = 539 (expected: 152808)\n",
      "Downloading reviews for appID = 1371580\n",
      "[appID = 1371580] expected #reviews = 2789\n",
      "[appID = 1371580] num_reviews = 345 (expected: 2789)\n",
      "Downloading reviews for appID = 1928980\n",
      "[appID = 1928980] expected #reviews = 7799\n",
      "[appID = 1928980] num_reviews = 1978 (expected: 7799)\n",
      "Downloading reviews for appID = 239140\n",
      "[appID = 239140] expected #reviews = 110576\n",
      "[appID = 239140] num_reviews = 319 (expected: 110576)\n",
      "Downloading reviews for appID = 848450\n",
      "[appID = 848450] expected #reviews = 49794\n",
      "[appID = 848450] num_reviews = 257 (expected: 49794)\n",
      "Downloading reviews for appID = 1504570\n",
      "[appID = 1504570] expected #reviews = 209\n",
      "[appID = 1504570] num_reviews = 100 (expected: 209)\n",
      "Downloading reviews for appID = 526870\n",
      "[appID = 526870] expected #reviews = 91167\n",
      "[appID = 526870] num_reviews = 423 (expected: 91167)\n",
      "Downloading reviews for appID = 962130\n",
      "[appID = 962130] expected #reviews = 38359\n",
      "[appID = 962130] num_reviews = 317 (expected: 38359)\n",
      "Downloading reviews for appID = 1284190\n",
      "[appID = 1284190] expected #reviews = 16453\n",
      "[appID = 1284190] num_reviews = 409 (expected: 16453)\n",
      "Downloading reviews for appID = 1259420\n",
      "[appID = 1259420] expected #reviews = 35653\n",
      "[appID = 1259420] num_reviews = 349 (expected: 35653)\n",
      "Downloading reviews for appID = 1307550\n",
      "[appID = 1307550] expected #reviews = 7768\n",
      "[appID = 1307550] num_reviews = 100 (expected: 7768)\n",
      "Downloading reviews for appID = 2080690\n",
      "[appID = 2080690] expected #reviews = 7439\n",
      "[appID = 2080690] num_reviews = 213 (expected: 7439)\n",
      "Downloading reviews for appID = 244850\n",
      "[appID = 244850] expected #reviews = 80399\n",
      "[appID = 244850] num_reviews = 193 (expected: 80399)\n",
      "Downloading reviews for appID = 383180\n",
      "[appID = 383180] expected #reviews = 3326\n",
      "[appID = 383180] num_reviews = 319 (expected: 3326)\n",
      "Downloading reviews for appID = 815370\n",
      "[appID = 815370] expected #reviews = 24449\n",
      "[appID = 815370] num_reviews = 215 (expected: 24449)\n",
      "Downloading reviews for appID = 2285150\n",
      "[appID = 2285150] expected #reviews = 3429\n",
      "[appID = 2285150] num_reviews = 160 (expected: 3429)\n",
      "Downloading reviews for appID = 361420\n",
      "[appID = 361420] expected #reviews = 53536\n",
      "[appID = 361420] num_reviews = 245 (expected: 53536)\n",
      "Downloading reviews for appID = 219740\n",
      "[appID = 219740] expected #reviews = 38872\n",
      "[appID = 219740] num_reviews = 124 (expected: 38872)\n",
      "Downloading reviews for appID = 371660\n",
      "[appID = 371660] expected #reviews = 11111\n",
      "[appID = 371660] num_reviews = 154 (expected: 11111)\n",
      "Downloading reviews for appID = 305620\n",
      "[appID = 305620] expected #reviews = 38780\n",
      "[appID = 305620] num_reviews = 197 (expected: 38780)\n",
      "Downloading reviews for appID = 529180\n",
      "[appID = 529180] expected #reviews = 5369\n",
      "[appID = 529180] num_reviews = 114 (expected: 5369)\n",
      "Downloading reviews for appID = 751780\n",
      "[appID = 751780] expected #reviews = 15570\n",
      "[appID = 751780] num_reviews = 143 (expected: 15570)\n",
      "Downloading reviews for appID = 393420\n",
      "[appID = 393420] expected #reviews = 6322\n",
      "[appID = 393420] num_reviews = 105 (expected: 6322)\n",
      "Downloading reviews for appID = 344760\n",
      "[appID = 344760] expected #reviews = 9664\n",
      "[appID = 344760] num_reviews = 118 (expected: 9664)\n",
      "Downloading reviews for appID = 768200\n",
      "[appID = 768200] expected #reviews = 3817\n",
      "[appID = 768200] num_reviews = 168 (expected: 3817)\n",
      "Downloading reviews for appID = 265550\n",
      "[appID = 265550] expected #reviews = 7241\n",
      "[appID = 265550] num_reviews = 118 (expected: 7241)\n",
      "Downloading reviews for appID = 280790\n",
      "[appID = 280790] expected #reviews = 14608\n",
      "[appID = 280790] num_reviews = 128 (expected: 14608)\n",
      "Downloading reviews for appID = 895400\n",
      "[appID = 895400] expected #reviews = 16158\n",
      "[appID = 895400] num_reviews = 174 (expected: 16158)\n",
      "Downloading reviews for appID = 250400\n",
      "[appID = 250400] expected #reviews = 5585\n",
      "[appID = 250400] num_reviews = 107 (expected: 5585)\n",
      "Downloading reviews for appID = 383120\n",
      "[appID = 383120] expected #reviews = 19585\n",
      "[appID = 383120] num_reviews = 297 (expected: 19585)\n",
      "Downloading reviews for appID = 290080\n",
      "[appID = 290080] expected #reviews = 5832\n",
      "[appID = 290080] num_reviews = 108 (expected: 5832)\n",
      "Downloading reviews for appID = 1159690\n",
      "[appID = 1159690] expected #reviews = 2501\n",
      "[appID = 1159690] num_reviews = 171 (expected: 2501)\n",
      "Downloading reviews for appID = 487120\n",
      "[appID = 487120] expected #reviews = 3496\n",
      "[appID = 487120] num_reviews = 106 (expected: 3496)\n",
      "Downloading reviews for appID = 543900\n",
      "[appID = 543900] expected #reviews = 3233\n",
      "[appID = 543900] num_reviews = 120 (expected: 3233)\n",
      "Downloading reviews for appID = 382310\n",
      "[appID = 382310] expected #reviews = 7186\n",
      "[appID = 382310] num_reviews = 143 (expected: 7186)\n",
      "Downloading reviews for appID = 738520\n",
      "[appID = 738520] expected #reviews = 5343\n",
      "Number of queries 150 reached. Cooldown: 310 seconds\n",
      "[appID = 738520] num_reviews = 128 (expected: 5343)\n",
      "Downloading reviews for appID = 573090\n",
      "[appID = 573090] expected #reviews = 34368\n",
      "[appID = 573090] num_reviews = 235 (expected: 34368)\n",
      "Downloading reviews for appID = 332500\n",
      "[appID = 332500] expected #reviews = 4146\n",
      "[appID = 332500] num_reviews = 105 (expected: 4146)\n",
      "Downloading reviews for appID = 313120\n",
      "[appID = 313120] expected #reviews = 23714\n",
      "[appID = 313120] num_reviews = 158 (expected: 23714)\n",
      "Downloading reviews for appID = 285920\n",
      "[appID = 285920] expected #reviews = 14055\n",
      "[appID = 285920] num_reviews = 120 (expected: 14055)\n",
      "Downloading reviews for appID = 715400\n",
      "[appID = 715400] expected #reviews = 1062\n",
      "[appID = 715400] num_reviews = 109 (expected: 1062)\n",
      "Downloading reviews for appID = 700030\n",
      "[appID = 700030] expected #reviews = 2594\n",
      "[appID = 700030] num_reviews = 146 (expected: 2594)\n",
      "Downloading reviews for appID = 1641960\n",
      "[appID = 1641960] expected #reviews = 2382\n",
      "[appID = 1641960] num_reviews = 128 (expected: 2382)\n",
      "Downloading reviews for appID = 299740\n",
      "[appID = 299740] expected #reviews = 15345\n",
      "[appID = 299740] num_reviews = 121 (expected: 15345)\n",
      "Downloading reviews for appID = 547860\n",
      "[appID = 547860] expected #reviews = 1005\n",
      "[appID = 547860] num_reviews = 101 (expected: 1005)\n",
      "Downloading reviews for appID = 402710\n",
      "[appID = 402710] expected #reviews = 8999\n",
      "[appID = 402710] num_reviews = 139 (expected: 8999)\n",
      "Downloading reviews for appID = 840800\n",
      "[appID = 840800] expected #reviews = 3249\n",
      "[appID = 840800] num_reviews = 119 (expected: 3249)\n",
      "Downloading reviews for appID = 1377380\n",
      "[appID = 1377380] expected #reviews = 3147\n",
      "[appID = 1377380] num_reviews = 134 (expected: 3147)\n",
      "Downloading reviews for appID = 349700\n",
      "[appID = 349700] expected #reviews = 4126\n",
      "[appID = 349700] num_reviews = 100 (expected: 4126)\n",
      "Downloading reviews for appID = 327070\n",
      "[appID = 327070] expected #reviews = 5302\n",
      "[appID = 327070] num_reviews = 130 (expected: 5302)\n",
      "Downloading reviews for appID = 1169040\n",
      "[appID = 1169040] expected #reviews = 6296\n",
      "[appID = 1169040] num_reviews = 151 (expected: 6296)\n",
      "Downloading reviews for appID = 361800\n",
      "[appID = 361800] expected #reviews = 2327\n",
      "[appID = 361800] num_reviews = 100 (expected: 2327)\n",
      "Downloading reviews for appID = 829590\n",
      "[appID = 829590] expected #reviews = 2022\n",
      "[appID = 829590] num_reviews = 107 (expected: 2022)\n",
      "Downloading reviews for appID = 2209150\n",
      "[appID = 2209150] expected #reviews = 397\n",
      "[appID = 2209150] num_reviews = 100 (expected: 397)\n",
      "Downloading reviews for appID = 897450\n",
      "[appID = 897450] expected #reviews = 2772\n",
      "[appID = 897450] num_reviews = 129 (expected: 2772)\n",
      "Downloading reviews for appID = 1072420\n",
      "[appID = 1072420] expected #reviews = 1758\n",
      "[appID = 1072420] num_reviews = 114 (expected: 1758)\n",
      "Downloading reviews for appID = 536270\n",
      "[appID = 536270] expected #reviews = 5418\n",
      "[appID = 536270] num_reviews = 161 (expected: 5418)\n",
      "Downloading reviews for appID = 758690\n",
      "[appID = 758690] expected #reviews = 969\n",
      "[appID = 758690] num_reviews = 111 (expected: 969)\n",
      "Downloading reviews for appID = 914620\n",
      "[appID = 914620] expected #reviews = 6638\n",
      "[appID = 914620] num_reviews = 139 (expected: 6638)\n",
      "Downloading reviews for appID = 707010\n",
      "[appID = 707010] expected #reviews = 3626\n",
      "[appID = 707010] num_reviews = 117 (expected: 3626)\n",
      "Downloading reviews for appID = 625340\n",
      "[appID = 625340] expected #reviews = 2216\n",
      "[appID = 625340] num_reviews = 104 (expected: 2216)\n",
      "Downloading reviews for appID = 1519090\n",
      "[appID = 1519090] expected #reviews = 269\n",
      "[appID = 1519090] num_reviews = 100 (expected: 269)\n",
      "Downloading reviews for appID = 360170\n",
      "[appID = 360170] expected #reviews = 2471\n",
      "[appID = 360170] num_reviews = 106 (expected: 2471)\n",
      "Downloading reviews for appID = 1501610\n",
      "[appID = 1501610] expected #reviews = 914\n",
      "[appID = 1501610] num_reviews = 102 (expected: 914)\n",
      "Downloading reviews for appID = 951440\n",
      "[appID = 951440] expected #reviews = 5158\n",
      "[appID = 951440] num_reviews = 119 (expected: 5158)\n",
      "Downloading reviews for appID = 526160\n",
      "[appID = 526160] expected #reviews = 2106\n",
      "[appID = 526160] num_reviews = 100 (expected: 2106)\n",
      "Downloading reviews for appID = 2197910\n",
      "[appID = 2197910] expected #reviews = 799\n",
      "[appID = 2197910] num_reviews = 107 (expected: 799)\n",
      "Downloading reviews for appID = 2313330\n",
      "[appID = 2313330] expected #reviews = 590\n",
      "[appID = 2313330] num_reviews = 188 (expected: 590)\n",
      "Downloading reviews for appID = 450860\n",
      "[appID = 450860] expected #reviews = 223\n",
      "[appID = 450860] num_reviews = 100 (expected: 223)\n",
      "Downloading reviews for appID = 1589120\n",
      "[appID = 1589120] expected #reviews = 575\n",
      "[appID = 1589120] num_reviews = 112 (expected: 575)\n",
      "Downloading reviews for appID = 1239430\n",
      "[appID = 1239430] expected #reviews = 568\n",
      "[appID = 1239430] num_reviews = 100 (expected: 568)\n",
      "Downloading reviews for appID = 307880\n",
      "[appID = 307880] expected #reviews = 4403\n",
      "[appID = 307880] num_reviews = 108 (expected: 4403)\n",
      "Downloading reviews for appID = 237870\n",
      "[appID = 237870] expected #reviews = 3824\n",
      "[appID = 237870] num_reviews = 104 (expected: 3824)\n",
      "Downloading reviews for appID = 366220\n",
      "[appID = 366220] expected #reviews = 2273\n",
      "[appID = 366220] num_reviews = 107 (expected: 2273)\n",
      "Downloading reviews for appID = 1967630\n",
      "[appID = 1967630] expected #reviews = 813\n",
      "[appID = 1967630] num_reviews = 111 (expected: 813)\n",
      "Downloading reviews for appID = 1360000\n",
      "[appID = 1360000] expected #reviews = 3273\n",
      "[appID = 1360000] num_reviews = 123 (expected: 3273)\n",
      "Downloading reviews for appID = 2262080\n",
      "[appID = 2262080] expected #reviews = 79\n",
      "[appID = 2262080] num_reviews = 78 (expected: 79)\n",
      "Downloading reviews for appID = 391730\n",
      "[appID = 391730] expected #reviews = 1802\n",
      "[appID = 391730] num_reviews = 104 (expected: 1802)\n",
      "Downloading reviews for appID = 280520\n",
      "[appID = 280520] expected #reviews = 987\n",
      "[appID = 280520] num_reviews = 101 (expected: 987)\n",
      "Downloading reviews for appID = 227860\n",
      "[appID = 227860] expected #reviews = 4794\n",
      "[appID = 227860] num_reviews = 111 (expected: 4794)\n",
      "Downloading reviews for appID = 1134700\n",
      "[appID = 1134700] expected #reviews = 580\n",
      "[appID = 1134700] num_reviews = 104 (expected: 580)\n",
      "Downloading reviews for appID = 418030\n",
      "[appID = 418030] expected #reviews = 6875\n",
      "[appID = 418030] num_reviews = 128 (expected: 6875)\n",
      "Downloading reviews for appID = 548480\n",
      "[appID = 548480] expected #reviews = 68\n",
      "[appID = 548480] num_reviews = 68 (expected: 68)\n",
      "Downloading reviews for appID = 544550\n",
      "[appID = 544550] expected #reviews = 3979\n",
      "[appID = 544550] num_reviews = 141 (expected: 3979)\n",
      "Downloading reviews for appID = 1635450\n",
      "[appID = 1635450] expected #reviews = 1532\n",
      "[appID = 1635450] num_reviews = 110 (expected: 1532)\n",
      "Game records written: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import steamreviews\n",
    "\n",
    "request_params = dict()\n",
    "# Reference: https://partner.steamgames.com/doc/store/getreviews\n",
    "request_params['filter'] = 'all'  # reviews are sorted by helpfulness instead of chronology\n",
    "request_params['language'] = 'english'\n",
    "request_params['day_range'] = '84'  # focus on reviews which were published during the past four weeks\n",
    "\n",
    "steamreviews.download_reviews_for_app_id_batch(chosen_request_params=request_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99218e2c-6096-4343-957c-1523f448e93b",
   "metadata": {},
   "source": [
    "### Merge Multiple JSON to one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170ce72c-2b92-400c-90cb-557f675abe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Directory containing the JSON files\n",
    "json_directory = os.getcwd()+\"/data/\"\n",
    "\n",
    "# Output CSV file path\n",
    "csv_file_path = 'output.csv'\n",
    "\n",
    "# Initialize an empty list to hold the extracted data\n",
    "data = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(json_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Load the JSON file\n",
    "            json_data = json.load(file)\n",
    "            # Extract reviews\n",
    "            for review_id, review_data in json_data['reviews'].items():\n",
    "                # Determine sentiment\n",
    "                sentiment = 'positive' if review_data['voted_up'] else 'negative'\n",
    "                # Extract the review text\n",
    "                review_text = review_data['review']\n",
    "                # Append the data to the list\n",
    "                data.append([sentiment, review_text])\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header\n",
    "    writer.writerow(['sentiment', 'review'])\n",
    "    # Write the data\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e97fa2-d40a-454b-b6f3-3623735869d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "807bb48f-9925-486f-aff8-22689815e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentiment', 'review'],\n",
      "        num_rows: 22348\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentiment', 'review'],\n",
      "        num_rows: 5587\n",
      "    })\n",
      "})\n",
      "{'input_ids': [101, 2028, 1997, 2026, 5440, 2399, 2000, 2377, 2894, 1012, 1024, 1006, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                            | 0/22348 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[0;32m     18\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_dicts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m tokenized_test \u001b[38;5;241m=\u001b[39m dataset_dicts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3158\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3543\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3544\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3545\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3546\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3547\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3551\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3556\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\datasets\\arrow_dataset.py:3416\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3415\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3416\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3418\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3419\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3420\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[64], line 18\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[1;32m---> 18\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2872\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2871\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2872\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2874\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2955\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2956\u001b[0m         )\n\u001b[0;32m   2957\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2979\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2980\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2996\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2997\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:3149\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3139\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3140\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3141\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3142\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3147\u001b[0m )\n\u001b[1;32m-> 3149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3151\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_fast.py:504\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    497\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m    498\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    502\u001b[0m )\n\u001b[1;32m--> 504\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    516\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[0;32m    518\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[0;32m    528\u001b[0m ]\n",
      "\u001b[1;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "df=reviews = pd.read_csv('output.csv', sep=',', header=0, names=[\"sentiment\", \"review\"])\n",
    "df.head\n",
    "# x=list(df['sentiment'])\n",
    "# y=list(df['review'])\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset_dicts = dataset.train_test_split(train_size=0.8, test_size=0.2, seed=42)\n",
    "print(dataset_dicts)\n",
    "print(tokenizer( dataset_dicts['train'][0][\"review\"]))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"review\"], truncation=True)\n",
    " \n",
    "tokenized_train = dataset_dicts['train'].map(preprocess_function, batched=True)\n",
    "tokenized_test = dataset_dicts['test'].map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1837a655-f329-4a11-adfd-7b81f1594061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPtorch",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
